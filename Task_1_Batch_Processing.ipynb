{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing \n",
    "In this project data from the Brussels Mobility Bike Counts API will be processed using PySpark. The devices are sensors located in Brussels to count the number of bikes passed by and measure their speed every 15 minutes. The data that will be used is historical data from 2018-12-06 to 2023-\n",
    "03-31. This can be done by reading csv from the URL provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.sql.types import StructField, StringType, StructType, IntegerType, FloatType\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Downloading the data\n",
    "- First, we gather the device names by querying the API, requesting the devices with `?request=devices` for all the required dates.\n",
    "- Then, we only select the device names and remove duplicates by building a set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices_full_url = 'https://data.mobility.brussels/bike/api/counts/?request=devices&startDate=20181206&endDate=20230331&outputFormat=csv'\n",
    "devices_full_df = pd.read_csv(devices_full_url)\n",
    "\n",
    "device_names = set(devices_full_df['Device name'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is seen that there are 18 sensors to be analyzed in the project. To download the data, we build a list from the set and iterate over the list. The aim is to store all the data in a single `data.csv` file. So we first open the file, and then we iterate over all the devices. For each, we make a request through the API and write the result in the big file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data from the 18 sensors and store it in a single file `data.csv`\n",
    "!mkdir SENSORS_DATA\n",
    "with open(f'SENSORS_DATA/data.csv', 'w', newline='') as f :\n",
    "  writer = csv.writer(f)\n",
    "  for device in list(device_names) :\n",
    "\n",
    "      # Formulate the URL\n",
    "      url = 'https://data.mobility.brussels/bike/api/counts/?request=history&featureID='+device+'&startDate=20181206&endDate=20230331&outputFormat=csv'\n",
    "      # Get the results from the request\n",
    "      response = requests.get(url)\n",
    "      # Format the answer\n",
    "      lines = response.text.splitlines()\n",
    "      header = lines[0] + \",Sensor\"\n",
    "      \n",
    "      # The header has been intentionnally removed.\n",
    "      # The structure of the files is `Date,Time gap,Count,Average speed,Sensor`\n",
    "      for line in lines[1:] :\n",
    "          line += f\",{device}\"\n",
    "          writer.writerow(line.split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling missing data\n",
    "It is seen that some sensors have several missing time stamps, so to complete the process they need to be added. Each day has 96 time gaps. A function to flatten the time gap by computing the number of days between 2018-12-06 and 2023-03-31 and setting time gap of 0 to the starting date then adding 96 to each day. By setting the time gap as index then reindexing the data we will have all the missing data and they can filled with count 0 and speed -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLATTEN TIME GAP\n",
    "# A pair 'day, time gap' is mapped to an absolute timegap, with timestamp 0 being '2018-12-06, 0'. \n",
    "# Each day is 96 time gaps.\n",
    "# So we first compute the number of days between '2018-12-06' and the current day\n",
    "# And we multiply this by 96 to have the number of elapsed time gaps\n",
    "# And we add the time gap of the day, which is the other element in the pair\n",
    "gaps_per_day = 96\n",
    "initialDayString = '2018-12-06'\n",
    "initialDay = datetime.strptime(initialDayString, '%Y-%m-%d')\n",
    "\n",
    "# Requires tgap to be integer, and 'day' to be a string \n",
    "def flatten_timestamp(day, tgap) :\n",
    "    day_delta = (datetime.strptime(day,'%Y-%m-%d') -initialDay).days\n",
    "    return tgap + day_delta*gaps_per_day\n",
    "\n",
    "# Works also in a single line\n",
    "# flatten_timestamp = lambda tgap : int(tgap[1]) + (datetime.strptime(str(tgap[0]),'%Y-%m-%d') -initialDay).days*gaps_per_day\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min and max timestamp\n",
    "max_timestamp = flatten_timestamp('2023-03-31', 96)\n",
    "min_timestamp = 0\n",
    "timestamp_range = range(min_timestamp, max_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>tgap</th>\n",
       "      <th>Count</th>\n",
       "      <th>Speed</th>\n",
       "      <th>DeviceName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-06</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>CJM90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-06</td>\n",
       "      <td>52</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>CJM90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-06</td>\n",
       "      <td>53</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>CJM90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-06</td>\n",
       "      <td>54</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>CJM90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-06</td>\n",
       "      <td>55</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>CJM90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          day  tgap  Count  Speed DeviceName\n",
       "0  2018-12-06    51      3     21      CJM90\n",
       "1  2018-12-06    52     23     19      CJM90\n",
       "2  2018-12-06    53     23     19      CJM90\n",
       "3  2018-12-06    54     12     17      CJM90\n",
       "4  2018-12-06    55     13     21      CJM90"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the csv and setting the columns\n",
    "df = pd.read_csv(\"SENSORS_DATA/data.csv\", header=None)\n",
    "df.columns = [\"day\", \"tgap\",\"Count\", \"Speed\", \"DeviceName\"]\n",
    "deviceNames = df[\"DeviceName\"].drop_duplicates().values\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1\n",
      "Dataset for sensor CJM90 loaded\n",
      "Processing 2\n",
      "Dataset for sensor CLW239 loaded\n",
      "Processing 3\n",
      "Dataset for sensor CEK18 loaded\n",
      "Processing 4\n",
      "Dataset for sensor CB1142 loaded\n",
      "Processing 5\n",
      "Dataset for sensor CEE016 loaded\n",
      "Processing 6\n",
      "Dataset for sensor CAT17 loaded\n",
      "Processing 7\n",
      "Dataset for sensor CB1699 loaded\n",
      "Processing 8\n",
      "Dataset for sensor CEK31 loaded\n",
      "Processing 9\n",
      "Dataset for sensor CEV011 loaded\n",
      "Processing 10\n",
      "Dataset for sensor CJE181 loaded\n",
      "Processing 11\n",
      "Dataset for sensor CB2105 loaded\n",
      "Processing 12\n",
      "Dataset for sensor CB1599 loaded\n",
      "Processing 13\n",
      "Dataset for sensor CVT387 loaded\n",
      "Processing 14\n",
      "Dataset for sensor CB1101 loaded\n",
      "Processing 15\n",
      "Dataset for sensor CEK049 loaded\n",
      "Processing 16\n",
      "Dataset for sensor CB02411 loaded\n",
      "Processing 17\n",
      "Dataset for sensor CB1143 loaded\n",
      "Processing 18\n",
      "Dataset for sensor COM205 loaded\n"
     ]
    }
   ],
   "source": [
    "!mkdir CLEAN_DATA\n",
    "i=0\n",
    "for device in deviceNames :\n",
    "  i += 1\n",
    "  print(f\"Processing {i}\")\n",
    "  \n",
    "  df_device = df[df[\"DeviceName\"] == device]\n",
    "  print(f\"Dataset for sensor {device} loaded\")\n",
    "\n",
    "  # Extracting all the time gaps from the dataframe in a single columnar dataframe\n",
    "  gaps_df = df_device.apply(lambda x : flatten_timestamp(x[0], x[1]), axis=1)\n",
    "\n",
    "  # Setting this column as the index of the dataframe\n",
    "  df_device.index = gaps_df\n",
    "\n",
    "  # Removing initial 'day, time gap' columns\n",
    "  df_device = df_device[[\"Count\",\"Speed\",\"DeviceName\"]]\n",
    "\n",
    "  # Time to fill missing lines with Pandas's 'reindex' method\n",
    "  # 'reindex' method can only fill with one value. Choosing 'np.NAN' so we\n",
    "  # can later fill np.NAN with our values\n",
    "  df_device = df_device.reindex(list(timestamp_range),fill_value=np.NAN)\n",
    "  \n",
    "  # Filling NANs accordingly\n",
    "# 0 count \n",
    "# -1 speed\n",
    "  df_device = df_device.fillna(value={\n",
    "      \"Count\":0,\n",
    "      \"Speed\":-1,\n",
    "      \"DeviceName\":device})\n",
    "# saving data files to CLEAN_DATA folder\n",
    "  df_device.to_csv(f\"CLEAN_DATA/{device}_v2.csv\", header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"8G\") \\\n",
    "    .appName(\"Batchprocessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the dataset of all sensor measures as an RDD\n",
    "device = sc.textFile(\"CLEAN_DATA/*\")\n",
    "# Splitting by comma and casting to specific types for each column\n",
    "device = device.map(lambda line : [int(line.split(',')[0]),float(line.split(',')[1]),float(line.split(',')[2]),line.split(',')[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in the files is **structured**, so we want to specify a schema. The task is about computing analytics, which requires aggregation operations, so we chose a **Spark DataFrame** as data structure (more suitable for aggregated queries than RDDs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the schema\n",
    "schema = StructType([ \\\n",
    "    StructField(\"tgap\",IntegerType(),True), \\\n",
    "    StructField(\"Count\",FloatType(),True), \\\n",
    "    StructField(\"Speed\",FloatType(),True), \\\n",
    "    StructField(\"DeviceName\", StringType(), True), \\\n",
    "  ])\n",
    "df = device.toDF(schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "# (optional) Verification of number of sensors considered\n",
    "\n",
    "# measures = 151392 # 151392 measures per device file.\n",
    "# # num = df.count()/measures\n",
    "# num2 = df.select(\"DeviceName\").distinct().count()\n",
    "# print(num2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute analytics, we rely on the `pyspark.sql.Window` object.\n",
    "\n",
    "We also use `pyspark.sql.functions`, for instance to compute the sum of a column (`F.sum`), ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each row, a window will be defined with the `rangeBetween` function, whose first argument is the lower bound of the window and the second argument is the upper bound.\n",
    "- Lower bound will be up to the first row of the dataframe, meaning the window takes \"everything that is before\"\n",
    "- Upper bound will be zero, so that no other rows after the timegap is considered\n",
    "\n",
    "The window also partitions by `DeviceName` for obvious reasons, because the analytics are to be computer for each device separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the window\n",
    "deviceWindow = Window.orderBy('tgap').partitionBy(\"DeviceName\")\\\n",
    "                    .rangeBetween(Window.unboundedPreceding, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the moving average\n",
    "The moving average is defined as $$\\bar{c_i}(t) = 1/t \\sum_{n = 0}^t c_i(n)$$\n",
    "\n",
    "We compute it by first computing for each row the cumulative count by using the `F.sum` function over the window defined above.\n",
    "\n",
    "Then, we divide the cumulative sum by the time gap `t`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the average\n",
    "## First cumulative sum\n",
    "df = df.withColumn('cum_sum', F.sum('Count').over(deviceWindow))\n",
    "## Then divide by the time gap, and drop the cumulative sum column\n",
    "df_analytics = df.withColumn('avg', df[\"cum_sum\"]/(df[\"tgap\"])).drop('cum_sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the standard deviation\n",
    "The definition of the standard deviation is\n",
    "\n",
    "$$ \\sigma_i(t) = \\sqrt{\\sum_{n=0}^t [c_i(n) - \\bar{c_i}(t)]^2}$$\n",
    "\n",
    "A first approach would be to compute for each row the deviation by subtracting the count by the average, as\n",
    "`df[\"Count] - df[\"avg\"]`, but this would be $c_i(n)-\\bar{c_i}(n)$, not $c_i(n)-\\bar{c_i}(t)$.\n",
    "\n",
    "To ensure we compute the right quantity, we used a trick to keep a track of the current row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the window of current row\n",
    "current_row_window = Window.partitionBy(\"DeviceName\").orderBy('tgap').rowsBetween(0, 0)\n",
    "# Extract the average of the first row of the window, which is the current row\n",
    "avg_current_row = F.first(F.col(\"avg\")).over(current_row_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analytics = df_analytics.withColumn(\"stddev\",\n",
    "    F.sqrt(\n",
    "        F.sum(\n",
    "            F.pow(df_analytics[\"Count\"]-avg_current_row, 2)     # Compute (c_i(n)-\\bar c_i(t))^2\n",
    "        ).over(window=deviceWindow)                             # Summing for all n\n",
    ")                                                               # And taking the square root\n",
    ")                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_analytics.orderBy(F.desc(\"avg\")).filter(df_analytics[\"tgap\"] < 128999).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the possible pairs (sensor1, sensor2)\n",
    "Using a simple join on the time gaps, and only keeping the pairs that have different sensor name by selecting `s1_Name < s2_Name` to ensure there are no duplicate pair `sensor1, sensor2 <-> sensor2, sensor1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_df = df_analytics.alias(\"sensor1\").join(df_analytics.alias(\"sensor2\"),\\\n",
    "                                   col(\"sensor1.tgap\") == col(\"sensor2.tgap\"))\\\n",
    "                                   .select(col(\"sensor1.tgap\"),\\\n",
    "                                           col(\"sensor1.DeviceName\").alias(\"s1_Name\"),\\\n",
    "                                           col(\"sensor1.Count\").alias(\"s1_Count\"),\\\n",
    "                                           col(\"sensor1.avg\").alias(\"s1_avg\"),\\\n",
    "                                           col(\"sensor1.stddev\").alias(\"s1_std\"),\\\n",
    "                                           col(\"sensor2.DeviceName\").alias(\"s2_Name\"),\\\n",
    "                                           col(\"sensor2.Count\").alias(\"s2_Count\"),\\\n",
    "                                           col(\"sensor2.avg\").alias(\"s2_avg\"),\\\n",
    "                                           col(\"sensor2.stddev\").alias(\"s2_std\")\\\n",
    "                                   )\\\n",
    "                                   .where(col(\"s1_Name\") < col(\"s2_Name\"))\\\n",
    "                                   .orderBy(\"tgap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the covariance\n",
    "For each row (each pair, each time gap), we now compute the covariance, with a similar technique as for the standard deviation.\n",
    "\n",
    "The covariance is defined as \n",
    "\n",
    "$$\\sigma_{i,j}(t) = \\sum_{n=0}^t (c_i(n)-\\bar{c_i}(t)) \\cdot (c_j(n)-\\bar{c_j}(t))$$\n",
    "\n",
    "For each row we will compute $(c_i(n)-\\bar{c_i}(t)) \\cdot (c_j(n)-\\bar{c_j}(t))$, and sum over a window as usual. To compute this quantity and avoid computing $(c_i(n)-\\bar{c_i}(\\underline{n})) \\cdot (c_j(n)-\\bar{c_j}(\\underline{n}))$, we will use the same trick as for computing the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a window limited to the current row\n",
    "current_row_window = Window.partitionBy(\"s1_Name\", \"s2_Name\").orderBy('tgap').rowsBetween(0, 0)\n",
    "\n",
    "# Calculates the constant value of s1_avg and s2_avg for the current row\n",
    "s1_avg_current_row = F.first(F.col(\"s1_avg\")).over(current_row_window)\n",
    "s2_avg_current_row = F.first(F.col(\"s2_avg\")).over(current_row_window)\n",
    "\n",
    "pairWindow = Window.partitionBy(\"s1_Name\", \"s2_Name\").orderBy('tgap').rangeBetween(Window.unboundedPreceding, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_covariance = pairs_df.withColumn(\"covariance\",\n",
    "                                       F.sum(\n",
    "                                        (F.col(\"s1_Count\") - s1_avg_current_row) * (F.col(\"s2_Count\") - s2_avg_current_row)\n",
    "                                       ).over(pairWindow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Pearson correlation coefficient\n",
    "The pearson correlation coefficient between two sensors $i$ and $j$ is defined for each time gap as \n",
    "\n",
    "$$\n",
    "r_{ij}(t) = \\frac{\\sigma_{i,j}(t)}{\\sigma_i(t)\\cdot \\sigma_j(t)}\\; ,\n",
    "$$\n",
    "where the $\\sigma_i$ are the moving standard deviation for each sensors as computed above, and the covariance $\\sigma_{i,j}$ is computed as above as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_correl = pairs_covariance.withColumn(\"pearsonCoeff\",\n",
    "                                           col(\"covariance\")/(col(\"s1_std\")*col(\"s2_std\"))\n",
    "                                           )\\\n",
    "                                         .orderBy(F.desc(\"pearsonCoeff\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_correl=pairs_correl.drop(\"s1_Count\",\"s1_avg\",\"s1_std\",\"s2_Count\",\"s2_avg\",\"s2_std\",\"covariance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------+------------------+\n",
      "|tgap|s1_Name|s2_Name|      pearsonCoeff|\n",
      "+----+-------+-------+------------------+\n",
      "|  51| CEK049|  CJM90|               1.0|\n",
      "|  51|CB02411| CB2105|               1.0|\n",
      "|  51| CB2105| COM205|               1.0|\n",
      "|  51| CB2105| CEK049|               1.0|\n",
      "|  51|CB02411| CEK049|               1.0|\n",
      "|  51|  CJM90| COM205|               1.0|\n",
      "|  51| CB2105|  CJM90|               1.0|\n",
      "|  51|CB02411|  CJM90|               1.0|\n",
      "|  51|CB02411| COM205|               1.0|\n",
      "|  51| CEK049| COM205|               1.0|\n",
      "| 440| CB1142| CB1143|               1.0|\n",
      "|  52|CB02411| CEK049| 0.998231052109537|\n",
      "|  52|CB02411| COM205|0.9964446914271927|\n",
      "|  52| CB2105| COM205|0.9920137015954205|\n",
      "|  53| CEK049|  CJM90|0.9902007318886551|\n",
      "|  52| CEK049| COM205|0.9896730715427468|\n",
      "|  54|CB02411| CEK049|0.9866565316667033|\n",
      "|  54|CB02411| COM205|0.9861355712508804|\n",
      "|  53|CB02411| CEK049|0.9847967175254008|\n",
      "|  53|CB02411| COM205|0.9838657312018031|\n",
      "+----+-------+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pairs_correl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the top correlated pairs\n",
    "- Partition by tgap and order by Pearson correlation coefficient\n",
    "- For each tgap, associate the rank to each row\n",
    "- Only keep rows with rank $\\leq 5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+------------------+\n",
      "|  tgap|s1_Name|s2_Name|      pearsonCoeff|\n",
      "+------+-------+-------+------------------+\n",
      "|151391| CB2105|  CJM90|0.8352454561723874|\n",
      "|151391| CB2105| CEK049|0.8172815203063362|\n",
      "|151391| CEK049|  CJM90|0.8133373740042474|\n",
      "|151391| CB1599| CEK049|0.8088956581175814|\n",
      "|151391|  CAT17| CVT387|0.7985076772625357|\n",
      "|151390| CB2105|  CJM90|0.8352459639225993|\n",
      "|151390| CB2105| CEK049|0.8172827939849917|\n",
      "|151390| CEK049|  CJM90|0.8133371912720383|\n",
      "|151390| CB1599| CEK049|0.8088952868085363|\n",
      "|151390|  CAT17| CVT387|0.7985073661930835|\n",
      "|151389| CB2105|  CJM90|0.8352457135708057|\n",
      "|151389| CB2105| CEK049|0.8172825203389336|\n",
      "|151389| CEK049|  CJM90|0.8133368503200527|\n",
      "|151389| CB1599| CEK049|0.8088949827441813|\n",
      "|151389|  CAT17| CVT387|0.7985072370714433|\n",
      "|151388| CB2105|  CJM90| 0.835245472079438|\n",
      "|151388| CB2105| CEK049|0.8172822510309377|\n",
      "|151388| CEK049|  CJM90|0.8133365203973153|\n",
      "|151388| CB1599| CEK049|0.8088944294256768|\n",
      "|151388|  CAT17| CVT387|0.7985071315380092|\n",
      "+------+-------+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the window partitioned by time gap and ordered by correlation coefficient in descending order\n",
    "window = Window.partitionBy(\"tgap\").orderBy(col(\"pearsonCoeff\").desc())\n",
    "\n",
    "# Add a new column with the rank of each row within the window\n",
    "ranked_df = pairs_correl.withColumn(\"rank\", F.rank().over(window))\n",
    "\n",
    "# Filter for the top-5 pairs of sensors for each time gap\n",
    "top_5_pairs = ranked_df.filter(col(\"rank\") <= 5).orderBy(F.desc(\"tgap\"), F.desc(\"pearsonCoeff\")) \\\n",
    "    .select(\"tgap\", \"s1_Name\", \"s2_Name\", \"pearsonCoeff\")\n",
    "\n",
    "\n",
    "top_5_pairs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
